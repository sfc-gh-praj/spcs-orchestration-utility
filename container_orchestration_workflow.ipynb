{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_creation"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "53d7d7c8-d6b2-4475-802c-f3e707b5d100",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Creating compute pools"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "Compute_Pools",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "-- Creating compute pools required for the service jobs\n\nshow image repositories;\n\nshow compute pools like 'pr%';\n\nCREATE COMPUTE POOL pr_std_pool_xs\n  MIN_NODES = 1\n  MAX_NODES = 1\n  INSTANCE_FAMILY = CPU_X64_XS;\n\nDESCRIBE COMPUTE POOL PR_STD_POOL_XS;\n\n\nCREATE COMPUTE POOL PR_STD_POOL_S\n  MIN_NODES = 1\n  MAX_NODES = 2\n  INSTANCE_FAMILY = CPU_X64_S;\n\nshow compute pools like 'PR_STD_POOL_S';\n\ngrant usage on compute pool pr_std_pool_xs to role SPCS_PSE_ROLE;\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4b14163d-675a-4c5b-9147-4817e046ac5d",
   "metadata": {
    "language": "sql",
    "name": "Viewing_CP_Status"
   },
   "outputs": [],
   "source": "show compute pools like 'PR_%';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4614545-9fd4-4f7f-b719-a9b62a07d76d",
   "metadata": {
    "language": "sql",
    "name": "Resume_Pool"
   },
   "outputs": [],
   "source": "-- Resuming the compute pools if they are suspended\nalter compute pool PR_STD_POOL_XS resume;\nalter compute pool PR_STD_POOL_S resume;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30833a6d-4204-4748-9710-7ddbd52baa82",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Creating logging tables and UDTF for tracking the tasks status for a DAG"
  },
  {
   "cell_type": "code",
   "id": "d59a0e7c-4185-4ea8-8762-7fe31fe48714",
   "metadata": {
    "language": "sql",
    "name": "create_logging_table",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- logging individual job status\ncreate or replace table jobs_run_stats( root_task_name string, task_name string, job_status string,GRAPH_RUN_ID string , graph_start_time timestamp_ltz, errors string, created_date datetime default current_timestamp());\n\n-- Tracking all tasks part of the task graph\ncreate table task_logging_stats (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,ERROR_MESSAGE VARCHAR);\n\n-- UDTF for getting the task status for the graph - TASK_GRAPH_RUN_STATS\ncreate or replace function TASK_GRAPH_RUN_STATS(ROOT_TASK_ID string, START_TIME timestamp_ltz)\n returns table (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,\n ERROR_MESSAGE VARCHAR)\nas\n$$\nselect\n        GRAPH_RUN_GROUP_ID,\n        NAME,\n        STATE,\n        RETURN_VALUE,\n        to_varchar(QUERY_START_TIME, 'YYYY-MM-DD HH24:MI:SS') as QUERY_START_TIME,\n        to_varchar(COMPLETED_TIME,'YYYY-MM-DD HH24:MI:SS') as COMPLETED_TIME,\n        timestampdiff('seconds', QUERY_START_TIME, COMPLETED_TIME) as DURATION,\n        ERROR_MESSAGE\n    from\n        table(INFORMATION_SCHEMA.TASK_HISTORY(\n              ROOT_TASK_ID => ROOT_TASK_ID ::string,\n              SCHEDULED_TIME_RANGE_START => START_TIME::timestamp_ltz,\n              SCHEDULED_TIME_RANGE_END => current_timestamp()\n      ))\n$$\n;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9dc1-2570-4396-9d54-a7f2c6028e50",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### Creating SP for calling the SPCS service job which spins up the container, runs it and terminates it\n\nThis code does the following :\n\n- Accepts the name of the service job to be created, pool name where the service jobs will be executed on along with some parameters which are the inputs to the container and the retry count which is used to identify how many time should the code retry to execute the container before gracefully terminating.\n\n- For every service job execution, we are tracking the status whether its Done or Failed and logging into jobs_run_stats table. It has details on the errors if any about the service job failures.\n\n- This SP is invoked from another SP create_job_tasks which creates the task DAG based on the job config file. \n"
  },
  {
   "cell_type": "code",
   "id": "4ec756b7-a57d-43d6-9b4e-5d00cdb385eb",
   "metadata": {
    "language": "sql",
    "name": "CreateJobService_spec",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n\nCREATE OR REPLACE PROCEDURE ExecuteJobService(service_name VARCHAR, pool_name VARCHAR,table_name VARCHAR,retry_count INT)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_job_service'\nAS\n$$\nfrom snowflake.snowpark.functions import col\nimport uuid\nimport re\nimport logging\nimport sys\n\nlogger = logging.getLogger(\"python_logger\")\n\ndef get_logger():\n    \"\"\"\n    Get a logger for local logging.\n    \"\"\"\n    logger = logging.getLogger(\"service-job\")\n    logger.setLevel(logging.INFO)\n    return logger\n\n# Functions which invokes the execute service job    \ndef execute_job(session, service_name, pool_name,table_name):\n   # Drop the existing service if it exists\n   session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n   session.sql(f'''\n                        EXECUTE JOB SERVICE\n                        IN COMPUTE POOL {pool_name}\n                        NAME={service_name}\n                        FROM SPECIFICATION  \n                        '\n                        spec:\n                         container:\n                         - name: main\n                           image: /pr_llmdemo/public/images/my_job_image:latest\n                           env:\n                             SNOWFLAKE_WAREHOUSE: xs_wh\n                           args:\n                           - \"--query=select current_time() as time,''hello''\"\n                           - \"--result_table={table_name}\"\n                        ';\n                    ''').collect()\n                    \n   job_status = session.sql(f'''\n                                    SELECT parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n                                ''').collect()[0]['STATUS']\n   return job_status\n    \n\n# This is the main function call invoked in the SP handler\n# This functin calls execute_job to run the container with all the parameters required.\ndef create_job_service(session, service_name, pool_name,table_name,retry_count):\n    import uuid\n    logger = get_logger()\n    logger.info(\"job_service\")\n    try:\n\n\n        cnt = retry_count\n        job_status = ''\n\n        # Execute the job service\n        logger.info(\n            f\"Executing the Job [{service_name}] on pool [{pool_name}]\"\n        )\n        job_status = execute_job(session, service_name, pool_name,table_name)\n\n        # Implementing retry mechanism. Fetching the retry count value from the config file per job\n        if job_status=='FAILED':\n            while(cnt >0):\n                r_cnt = retry_count+1 - cnt\n                logger.info(\n                                f\"Retrying Executing the Job [{service_name}] on pool [{pool_name}] - [{r_cnt}]  out of {retry_count} times \"\n                            )\n                job_status = execute_job(session, service_name, pool_name,table_name)\n                if job_status == 'DONE':\n                    break\n                cnt = cnt - 1\n                \n        \n        if job_status=='FAILED':\n            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n            ''').collect()[0]['LOGS'])\n        else:\n            job_errors = ''\n\n        # Getting the DAG Task details. SYSTEM$TASK_RUNTIME_INFO can only work inside a task.\n        result = session.sql(\"\"\"select\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_NAME')\n                                root_task_name,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_NAME') \n                                task_name,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID')\n                                run_id,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP')                  dag_start_time\n                            \n                            \"\"\").collect()[0]\n       \n        current_root_task_name, current_task_name ,current_graph_run_id , current_graph_start_time = result.ROOT_TASK_NAME, result.TASK_NAME ,result.RUN_ID, result.DAG_START_TIME\n        \n        #'a','b','c','2024-01-01'\n        \n        #result.ROOT_TASK_NAME, result.TASK_NAME ,result.RUN_ID, result.DAG_START_TIME\n        \n        # Inserting job status into logging table\n        _ = session.sql(f'''\n        INSERT INTO jobs_run_stats \n        (root_task_name,task_name,graph_run_id ,job_status,graph_start_time, errors ,created_date)\n        SELECT '{current_root_task_name}'\n        ,'{current_task_name}'\n        ,'{current_graph_run_id}'\n        ,'{job_status}'\n        ,'{current_graph_start_time}'\n        , '{job_errors}'\n        ,current_timestamp()\n        ''').collect()\n\n        \n        return job_status\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        if job_status=='FAILED':\n            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n            ''').collect()[0]['LOGS'])\n        else:\n            job_errors = ''\n        \n        session.sql(f\"\"\"\n           INSERT INTO jobs_run_stats(task_name,errors,graph_run_id,job_status,created_date)\n           SELECT '{service_name}',\n           '{job_errors}',\n           '{current_graph_run_id}',\n           '{job_status}',\n           current_timestamp()\n            \n                    \"\"\").collect()\n                    \n        return f'Error Occured.. Refer the logs - {e}'\n                   \n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6ee36ca-e243-4696-aec9-a8b65f1d6db4",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "### SP to create Snowflake task DAG - Fan-out and Fan-in Workflow Implementation\n\nThe code has the logic which creates the fan-in and fan-out workflow and does the following tasks:\n- Based on the config file passed during its invocation it will create the task DAG (fan-out and fan-in scenario) and calls the SP created above with parameters fetched from the config file. The root task DAG is scheduled to run every 59 mins.\n- Every task execution has the dependency on a specific task. Example T1 is dependent on root_task, T2 is dependent on root and T3 is dependent on T1 which implements the dependency workflow that is required.\n- This code creates a finalizer task which tracks the status of all the tasks( failure or Success) and logs it into the table task_logging_stats. "
  },
  {
   "cell_type": "code",
   "id": "ddba4376-7eb6-4b58-aac2-e2cb3e3d7b73",
   "metadata": {
    "language": "sql",
    "name": "sp_create_job_tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE create_job_tasks(file_name string)\nRETURNS string\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_jobservice_tasks'\nAS\n$$\nfrom snowflake.snowpark.files import SnowflakeFile\nimport json\n\ndef create_jobservice_tasks(session, file_name):\n  parent_task_name = 'root_task'\n  parent_task_sql = f'''CREATE OR REPLACE TASK {parent_task_name} \n              USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL' \n              SCHEDULE = '59 MINUTE' \n      AS\n      SELECT CURRENT_TIMESTAMP() ;'''\n\n  session.sql(f'''{parent_task_sql}''').collect()\n  print(parent_task_sql)\n\n\n  with SnowflakeFile.open(file_name) as j:\n      json_data= json.load(j)\n\n\n  for idx, task in enumerate(json_data):\n      task_name = task['task_name']\n      after_task_name = task['after_task_name']\n      task_sql = f\"CREATE  OR REPLACE TASK {task_name} \"\n      task_sql += f\"  WAREHOUSE = xs_wh \"\n      task_sql += f\"  AFTER {after_task_name}  \"\n      task_sql += f\" AS CALL ExecuteJobService('{task['job_name']}','{task['compute_pool_name']}','{task['table_name']}',{task['retry_count']})\"\n      # logger.info(f'{task_sql}')\n      session.sql(f'''{task_sql}''').collect()\n\n      print(task_sql)\n\n\n  session.sql(f\"\"\"\n              create or replace task GET_GRAPH_STATS\n  warehouse = 'xs_wh'\n  finalize = 'root_task'\n  as\n    declare\n      ROOT_TASK_ID string;\n      START_TIME timestamp_ltz;\n      \n    begin\n      ROOT_TASK_ID := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_UUID'));\n\n      START_TIME := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP'));\n\n      -- Insert into the logging table\n      INSERT INTO task_logging_stats(GRAPH_RUN_GROUP_ID , NAME ,  STATE  , RETURN_VALUE ,QUERY_START_TIME ,COMPLETED_TIME , DURATION_IN_SECS ,\n                                      ERROR_MESSAGE \n                                    )\n      SELECT * FROM TABLE(TASK_GRAPH_RUN_STATS(:ROOT_TASK_ID, :START_TIME))  where NAME !='GET_GRAPH_STATS';\n\n    end;\n              \"\"\"\n              ).collect()\n\n  session.sql('alter task GET_GRAPH_STATS resume').collect()\n  \n  session.sql(f'''SELECT SYSTEM$TASK_DEPENDENTS_ENABLE('root_task')''').collect()\n\n  return 'done'\n\n$$;\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead53f49-080e-4f73-9c4d-9f857fa6d454",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Creating stage for config.json\nCREATE OR REPLACE STAGE JOBS DIRECTORY = (\n    ENABLE = true);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4c633cc-e701-406c-bf3b-aef77c94eb85",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Upload the jobconfig.json file to JOBS stage\nls @jobs",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11843d47-b64e-4ae7-ad12-0a5b5f96e869",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "### Calling the Scheduling workflow SP\n\nIn the below cell we are invoking the scheduling workflow SP which accepts the jobconfig file which has all the  config required for the tasks to be created. Below is an extract from the config file. \n\n[{\n    \"task_name\":\"t_myjob_1\",\n    \"compute_pool_name\":\"PR_STD_POOL_XS\",\n    \"job_name\":\"myjob_1\",\n    \"table_name\":\"results_1\",\n    \"retry_count\":0,\n    \"after_task_name\":\"root_task\"\n   },\n   \n\n]"
  },
  {
   "cell_type": "code",
   "id": "285312e4-35ed-4056-aed4-235138c2ccfb",
   "metadata": {
    "language": "sql",
    "name": "creating_job_tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ncall create_job_tasks(build_scoped_file_url(@jobs, 'jobconfig.json'));\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb69ce30-4b3e-4b08-bd4e-77b007d12557",
   "metadata": {
    "language": "sql",
    "name": "View_Tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--  Checks the DAG task created for the root_task. You can see the column predecessor which mentions the dependent task name\nselect *\n  from table(information_schema.task_dependents(task_name => 'root_task', recursive => true));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1114e19c-d968-42f1-8d79-286ca85e8c70",
   "metadata": {
    "language": "sql",
    "name": "Suspending_Resuming_Root_Task",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- alter task root_task suspend;\n-- alter task root_task resume;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e10d7c6-d664-4d32-bc48-37c1b26b1345",
   "metadata": {
    "language": "sql",
    "name": "simulating_failures"
   },
   "outputs": [],
   "source": "--  With the below code we are simulating failure, so that we can tracjk what error are we tracking in the logging tables and to test the retry logic behaviour\n\n-- This should fail the job3\nALTER TABLE RESULTS_3 DROP COLUMN \"'HELLO'\";\nALTER TABLE RESULTS_3 ADD COLUMN CREATEDATE DATETIME ;\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2f38a8e-45dd-4e87-8dd6-5dcc54861c2a",
   "metadata": {
    "language": "sql",
    "name": "query_job_run_stats",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "--  View job run status\nselect top 10 * from jobs_run_stats order by created_date desc;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "416c9fab-b452-48c1-9ef2-e351189f3888",
   "metadata": {
    "language": "sql",
    "name": "query_task_logging_stats",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--  Query task logging status (by the finalizer task)\nSELECT top 10 * FROM task_logging_stats ORDER BY CAST(QUERY_START_TIME AS DATETIME) DESC;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4944b38c-c80d-4dd7-bed5-88ad6bdd4416",
   "metadata": {
    "language": "sql",
    "name": "view_dependent_tasks"
   },
   "outputs": [],
   "source": "--  View all dependent task for root_task\nselect *\n  from table(information_schema.task_dependents(task_name => 'root_task', recursive => false));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b1fdb61-bb11-49d3-8672-80745a14589d",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "#### This is the SP to drop all the tasks whose root task name is root_task."
  },
  {
   "cell_type": "code",
   "id": "c5fbb09f-d267-4e1f-babc-7b74e4988b23",
   "metadata": {
    "language": "sql",
    "name": "sp_drop_job_tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE PROCEDURE drop_job_tasks()\nRETURNS string\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'drop_tasks'\nexecute as caller\nAS\n$$\nfrom snowflake.snowpark.files import SnowflakeFile\nimport json\ndef drop_tasks(session):\n    session.sql('alter task root_task suspend').collect()\n    res= session.sql(f''' select name\n        from table(information_schema.task_dependents(task_name => 'root_task', recursive => true))''').collect()\n    for r in res:\n        print(r.NAME)\n        session.sql(f'drop task {r.NAME}').collect()\n    session.sql('drop task GET_GRAPH_STATS').collect()\n    return 'Done'\n$$;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f701eb5-44e1-4c3d-aae3-782a970532b2",
   "metadata": {
    "language": "sql",
    "name": "Dropping_Tasks",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Deleting the DAG\ncall drop_job_tasks();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06dc409e-07a4-4fe2-9977-4162bab80b5c",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### Below code is show can you use YAML file for creating the service jobs. "
  },
  {
   "cell_type": "code",
   "id": "381bfdd3-a483-49d3-ac8c-01d72cc7dacf",
   "metadata": {
    "language": "python",
    "name": "CreateJobService_using_yaml",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n\nCREATE OR REPLACE PROCEDURE CreateJobService(service_name VARCHAR, pool_name VARCHAR)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_job_service'\nAS\n$$\nfrom snowflake.snowpark.functions import col\nimport uuid\n\n\ndef create_job_service(session, service_name, pool_name):\n    import uuid\n    try:\n        # Drop the existing service if it exists\n        session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n        \n        # Execute the job service\n        session.sql(f'''\n                        EXECUTE JOB SERVICE\n                        IN COMPUTE POOL {pool_name}\n                        NAME={service_name}\n                        FROM @specs\n                        SPECIFICATION_FILE='my_job_spec.yaml' \n                    ''').collect()\n        \n        # Get the status of the job service\n        job_status = session.sql(f'''\n                                    SELECT parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n                                ''').collect()[0]['STATUS']\n        return job_status\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 'Error Occured.. Refer the logs'\n        \n                     \n$$;",
   "execution_count": null
  }
 ]
}