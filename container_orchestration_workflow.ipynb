{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_creation"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7d7c8-d6b2-4475-802c-f3e707b5d100",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "### Creating compute pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "Compute_Pools",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Creating compute pools required for the service jobs\n",
    "\n",
    "use role accountadmin;\n",
    "\n",
    "CREATE COMPUTE POOL pr_std_pool_xs\n",
    "  MIN_NODES = 1\n",
    "  MAX_NODES = 1\n",
    "  INSTANCE_FAMILY = CPU_X64_XS;\n",
    "\n",
    "DESCRIBE COMPUTE POOL PR_STD_POOL_XS;\n",
    "\n",
    "CREATE COMPUTE POOL PR_STD_POOL_S\n",
    "  MIN_NODES = 1\n",
    "  MAX_NODES = 2\n",
    "  INSTANCE_FAMILY = CPU_X64_S;\n",
    "\n",
    "show compute pools like 'PR_STD_POOL_S';\n",
    "\n",
    "-- You can use any role that you have created instead of SPCS_PSE_ROLE\n",
    "grant usage on compute pool pr_std_pool_xs to role SPCS_PSE_ROLE;\n",
    "grant usage on compute pool pr_std_pool_s to role SPCS_PSE_ROLE;\n",
    "\n",
    "use role SPCS_PSE_ROLE;\n",
    "\n",
    "CREATE OR REPLACE STAGE JOBS DIRECTORY = (\n",
    "    ENABLE = true);\n",
    "\n",
    "CREATE IMAGE REPOSITORY IF NOT EXISTS IMAGES;\n",
    "\n",
    "show image repositories;\n",
    "\n",
    "show compute pools like 'pr%';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14163d-675a-4c5b-9147-4817e046ac5d",
   "metadata": {
    "language": "sql",
    "name": "Viewing_CP_Status",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "show compute pools like 'PR_%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4614545-9fd4-4f7f-b719-a9b62a07d76d",
   "metadata": {
    "language": "sql",
    "name": "Resume_Pool",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Resuming the compute pools if they are suspended\n",
    "alter compute pool PR_STD_POOL_XS resume;\n",
    "alter compute pool PR_STD_POOL_S resume;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30833a6d-4204-4748-9710-7ddbd52baa82",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "### Creating logging tables and UDTF for tracking the tasks status for a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a0e7c-4185-4ea8-8762-7fe31fe48714",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "create_logging_table",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- logging individual job status\n",
    "create or replace table jobs_run_stats( root_task_name string, task_name string, job_status string,GRAPH_RUN_ID string , graph_start_time timestamp_ltz, errors string, created_date datetime default current_timestamp());\n",
    "\n",
    "-- Tracking all tasks part of the task graph\n",
    "create table task_logging_stats (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,ERROR_MESSAGE VARCHAR);\n",
    "\n",
    "-- UDTF for getting the task status for the graph - TASK_GRAPH_RUN_STATS\n",
    "create or replace function TASK_GRAPH_RUN_STATS(ROOT_TASK_ID string, START_TIME timestamp_ltz)\n",
    " returns table (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,\n",
    " ERROR_MESSAGE VARCHAR)\n",
    "as\n",
    "$$\n",
    "select\n",
    "        GRAPH_RUN_GROUP_ID,\n",
    "        NAME,\n",
    "        STATE,\n",
    "        RETURN_VALUE,\n",
    "        to_varchar(QUERY_START_TIME, 'YYYY-MM-DD HH24:MI:SS') as QUERY_START_TIME,\n",
    "        to_varchar(COMPLETED_TIME,'YYYY-MM-DD HH24:MI:SS') as COMPLETED_TIME,\n",
    "        timestampdiff('seconds', QUERY_START_TIME, COMPLETED_TIME) as DURATION,\n",
    "        ERROR_MESSAGE\n",
    "    from\n",
    "        table(INFORMATION_SCHEMA.TASK_HISTORY(\n",
    "              ROOT_TASK_ID => ROOT_TASK_ID ::string,\n",
    "              SCHEDULED_TIME_RANGE_START => START_TIME::timestamp_ltz,\n",
    "              SCHEDULED_TIME_RANGE_END => current_timestamp()\n",
    "      ))\n",
    "$$\n",
    ";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9dc1-2570-4396-9d54-a7f2c6028e50",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "### Creating SP for calling the SPCS service job which spins up the container, runs it and terminates it\n",
    "\n",
    "This code does the following :\n",
    "\n",
    "- Accepts the name of the service job to be created, pool name where the service jobs will be executed on along with some parameters which are the inputs to the container and the retry count which is used to identify how many time should the code retry to execute the container before gracefully terminating.\n",
    "\n",
    "- For every service job execution, we are tracking the status whether its Done or Failed and logging into jobs_run_stats table. It has details on the errors if any about the service job failures.\n",
    "\n",
    "- This SP is invoked from another SP create_job_tasks which creates the task DAG based on the job config file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec756b7-a57d-43d6-9b4e-5d00cdb385eb",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "CreateJobService_spec",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use role SPCS_PSE_ROLE;\n",
    "\n",
    "CREATE OR REPLACE PROCEDURE ExecuteJobService(service_name VARCHAR, image_name VARCHAR, pool_name VARCHAR,table_name VARCHAR,retry_count INT)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "HANDLER = 'create_job_service'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.functions import col\n",
    "import uuid\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(\"python_logger\")\n",
    "\n",
    "def get_logger():\n",
    "    \"\"\"\n",
    "    Get a logger for local logging.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"service-job\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "# Functions which invokes the execute service job    \n",
    "def execute_job(session, service_name, image_name,pool_name,table_name):\n",
    "   # Drop the existing service if it exists\n",
    "   session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n",
    "   sql_qry=f'''\n",
    "                        EXECUTE JOB SERVICE\n",
    "                        IN COMPUTE POOL {pool_name}\n",
    "                        NAME={service_name}\n",
    "                        FROM SPECIFICATION  \n",
    "                        '\n",
    "                        spec:\n",
    "                         container:\n",
    "                         - name: main\n",
    "                           image: {image_name}\n",
    "                           env:\n",
    "                             SNOWFLAKE_WAREHOUSE: xs_wh\n",
    "                           args:\n",
    "                           - \"--query=select current_time() as time,''hello''\"\n",
    "                           - \"--result_table={table_name}\"\n",
    "                        ';\n",
    "                    '''\n",
    "   #print(sql_qry)\n",
    "   \n",
    "   try: \n",
    "    _=session.sql(sql_qry).collect()\n",
    "    \n",
    "   except Exception as e:        \n",
    "    logger.error(f\"An error occurred running the app in the container: {e}\")\n",
    "    \n",
    "   finally:\n",
    "                \n",
    "    job_status = session.sql(f''' SELECT    parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n",
    "                                ''').collect()[0]['STATUS']\n",
    "\n",
    "    return job_status\n",
    "\n",
    "# This is the main function call invoked in the SP handler\n",
    "# This functin calls execute_job to run the container with all the parameters required.\n",
    "def create_job_service(session, service_name, image_name,pool_name,table_name,retry_count):\n",
    "    import uuid\n",
    "    logger = get_logger()\n",
    "    logger.info(\"job_service\")\n",
    "    job_status = ''\n",
    "    job_errors = ''\n",
    "    current_root_task_name = ''\n",
    "    current_task_name = ''\n",
    "    current_graph_run_id = ''\n",
    "    current_graph_start_time = ''\n",
    "    try:\n",
    "\n",
    "        cnt = retry_count\n",
    "\n",
    "        # Execute the job service\n",
    "        logger.info(\n",
    "            f\"Executing the Job [{service_name}] on pool [{pool_name}]\"\n",
    "        )\n",
    "        job_status = execute_job(session, service_name,image_name, pool_name,table_name)\n",
    "\n",
    "        # Implementing retry mechanism. Fetching the retry count value from the config file per job\n",
    "        if job_status=='FAILED':\n",
    "            while(cnt >0):\n",
    "                r_cnt = retry_count+1 - cnt\n",
    "                logger.info(\n",
    "                                f\"Retrying Executing the Job [{service_name}] on pool [{pool_name}] - [{r_cnt}]  out of {retry_count} times \"\n",
    "                            )\n",
    "                job_status =  execute_job(session, service_name,image_name, pool_name,table_name)\n",
    "                if job_status == 'DONE':\n",
    "                    break\n",
    "                cnt = cnt - 1\n",
    "                \n",
    "        \n",
    "        if job_status=='FAILED':\n",
    "            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n",
    "            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n",
    "            ''').collect()[0]['LOGS'])\n",
    "        else:\n",
    "            job_errors = ''\n",
    "\n",
    "        # Getting the DAG Task details. SYSTEM$TASK_RUNTIME_INFO can only work inside a task.\n",
    "        result = session.sql(\"\"\"select\n",
    "                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_NAME')\n",
    "                                root_task_name,\n",
    "                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_NAME') \n",
    "                                task_name,\n",
    "                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID')\n",
    "                                run_id,\n",
    "                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP')  dag_start_time\n",
    "                            \n",
    "                            \"\"\").collect()[0]\n",
    "                            \n",
    "        current_root_task_name = result.ROOT_TASK_NAME\n",
    "        current_task_name = result.TASK_NAME\n",
    "        current_graph_run_id = result.RUN_ID\n",
    "        current_graph_start_time = result.DAG_START_TIME\n",
    "               \n",
    "        #'a','b','c','2024-01-01'\n",
    "        \n",
    "        #result.ROOT_TASK_NAME, result.TASK_NAME ,result.RUN_ID, result.DAG_START_TIME\n",
    "        \n",
    "        # Inserting job status into logging table\n",
    "        _ = session.sql(f'''\n",
    "        INSERT INTO jobs_run_stats \n",
    "        (root_task_name,task_name,graph_run_id ,job_status,graph_start_time, errors ,created_date)\n",
    "        SELECT '{current_root_task_name}'\n",
    "        ,'{current_task_name}'\n",
    "        ,'{current_graph_run_id}'\n",
    "        ,'{job_status}'\n",
    "        ,'{current_graph_start_time}'\n",
    "        ,'{job_errors}'\n",
    "        ,current_timestamp()\n",
    "        ''').collect()\n",
    "\n",
    "        \n",
    "        return job_status\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if job_status=='FAILED':\n",
    "            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n",
    "            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n",
    "            ''').collect()[0]['LOGS'])\n",
    "        else:\n",
    "            job_errors = ''\n",
    "        \n",
    "        session.sql(f\"\"\"\n",
    "           INSERT INTO jobs_run_stats(task_name,errors,graph_run_id,job_status,created_date)\n",
    "           SELECT '{service_name}',\n",
    "           '{job_errors}',\n",
    "           '{current_graph_run_id}',\n",
    "           '{job_status}',\n",
    "           current_timestamp()\n",
    "            \n",
    "                    \"\"\").collect()\n",
    "                    \n",
    "        return f'Error Occured.. Refer the job error column - {e}'\n",
    "                   \n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee36ca-e243-4696-aec9-a8b65f1d6db4",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "### SP to create Snowflake task DAG - Fan-out and Fan-in Workflow Implementation\n",
    "\n",
    "The code has the logic which creates the fan-in and fan-out workflow and does the following tasks:\n",
    "- Based on the config file passed during its invocation it will create the task DAG (fan-out and fan-in scenario) and calls the SP created above with parameters fetched from the config file. The root task DAG is scheduled to run every 59 mins.\n",
    "- Every task execution has the dependency on a specific task. Example T1 is dependent on root_task, T2 is dependent on root and T3 is dependent on T1 which implements the dependency workflow that is required.\n",
    "- This code creates a finalizer task which tracks the status of all the tasks( failure or Success) and logs it into the table task_logging_stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba4376-7eb6-4b58-aac2-e2cb3e3d7b73",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "sp_create_job_tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "use role SPCS_PSE_ROLE;\n",
    "\n",
    "CREATE OR REPLACE PROCEDURE create_job_tasks(file_name string)\n",
    "RETURNS string\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "HANDLER = 'create_jobservice_tasks'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "import json\n",
    "\n",
    "def create_jobservice_tasks(session, file_name):\n",
    "  parent_task_name = 'root_task'\n",
    "  parent_task_sql = f'''CREATE OR REPLACE TASK {parent_task_name} \n",
    "              USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL' \n",
    "              SCHEDULE = '59 MINUTE' \n",
    "      AS\n",
    "      SELECT CURRENT_TIMESTAMP() ;'''\n",
    "\n",
    "  session.sql(f'''{parent_task_sql}''').collect()\n",
    "  print(parent_task_sql)\n",
    "\n",
    "  with SnowflakeFile.open(file_name) as j:\n",
    "      json_data= json.load(j)\n",
    "\n",
    "  for idx, task in enumerate(json_data):\n",
    "      task_name = task['task_name']\n",
    "      after_task_name = task['after_task_name']\n",
    "      task_sql = f\"CREATE  OR REPLACE TASK {task_name} \"\n",
    "      task_sql += f\"  WAREHOUSE = xs_wh \"\n",
    "      task_sql += f\"  AFTER {after_task_name}  \"\n",
    "      task_sql += f\" AS CALL ExecuteJobService('{task['job_name']}','{task['image_name']}','{task['compute_pool_name']}','{task['table_name']}',{task['retry_count']})\"\n",
    "      # logger.info(f'{task_sql}')\n",
    "      session.sql(f'''{task_sql}''').collect()\n",
    "\n",
    "      print(task_sql)\n",
    "\n",
    "  # This is the Finalize task which gets the status for every task part of the DAG and loads into task_logging_stats table\n",
    "  session.sql(f\"\"\"\n",
    "              create or replace task GET_GRAPH_STATS\n",
    "  warehouse = 'xs_wh'\n",
    "  finalize = 'root_task'\n",
    "  as\n",
    "    declare\n",
    "      ROOT_TASK_ID string;\n",
    "      START_TIME timestamp_ltz;\n",
    "      \n",
    "    begin\n",
    "      ROOT_TASK_ID := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_UUID'));\n",
    "\n",
    "      START_TIME := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP'));\n",
    "\n",
    "      -- Insert into the logging table\n",
    "      INSERT INTO task_logging_stats(GRAPH_RUN_GROUP_ID , NAME ,  STATE  , RETURN_VALUE ,QUERY_START_TIME ,COMPLETED_TIME , DURATION_IN_SECS ,\n",
    "                                      ERROR_MESSAGE \n",
    "                                    )\n",
    "      SELECT * FROM TABLE(TASK_GRAPH_RUN_STATS(:ROOT_TASK_ID, :START_TIME))  where NAME !='GET_GRAPH_STATS';\n",
    "\n",
    "    end;\n",
    "              \"\"\"\n",
    "              ).collect()\n",
    "\n",
    "  session.sql('alter task GET_GRAPH_STATS resume').collect()\n",
    "  \n",
    "  session.sql(f'''SELECT SYSTEM$TASK_DEPENDENTS_ENABLE('root_task')''').collect()\n",
    "\n",
    "  return 'done'\n",
    "\n",
    "$$;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead53f49-080e-4f73-9c4d-9f857fa6d454",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell6",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Creating stage for config.json\n",
    "CREATE OR REPLACE STAGE JOBS DIRECTORY = (\n",
    "    ENABLE = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c633cc-e701-406c-bf3b-aef77c94eb85",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell3",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Upload the jobconfig.json file to JOBS stage\n",
    "ls @jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11843d47-b64e-4ae7-ad12-0a5b5f96e869",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "### Calling the Scheduling workflow SP\n",
    "\n",
    "In the below cell we are invoking the scheduling workflow SP which accepts the jobconfig file which has all the  config required for the tasks to be created. Below is an extract from the config file. \n",
    "\n",
    "[{\n",
    "    \"task_name\":\"t_myjob_1\",\n",
    "    \"compute_pool_name\":\"PR_STD_POOL_XS\",\n",
    "    \"job_name\":\"myjob_1\",\n",
    "    \"table_name\":\"results_1\",\n",
    "    \"retry_count\":0,\n",
    "    \"after_task_name\":\"root_task\"\n",
    "   },\n",
    "   \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285312e4-35ed-4056-aed4-235138c2ccfb",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "creating_job_tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "call create_job_tasks(build_scoped_file_url(@jobs, 'jobconfig.json'));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ce30-4b3e-4b08-bd4e-77b007d12557",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "View_Tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  Checks the DAG task created for the root_task. You can see the column predecessor which mentions the dependent task name\n",
    "select *\n",
    "  from table(information_schema.task_dependents(task_name => 'root_task', recursive => true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10d7c6-d664-4d32-bc48-37c1b26b1345",
   "metadata": {
    "language": "sql",
    "name": "simulating_failures",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  With the below code we are simulating failure, so that we can tracjk what error are we tracking in the logging tables and to test the retry logic behaviour\n",
    "\n",
    "-- This should fail the job3\n",
    "ALTER TABLE RESULTS_3 DROP COLUMN \"'HELLO'\";\n",
    "ALTER TABLE RESULTS_3 ADD COLUMN CREATEDATE DATETIME ;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f38a8e-45dd-4e87-8dd6-5dcc54861c2a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "query_job_run_stats",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  View job run status\n",
    "select top 10 * from jobs_run_stats order by created_date desc;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c9fab-b452-48c1-9ef2-e351189f3888",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "query_task_logging_stats",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  Query task logging status (by the finalizer task)\n",
    "SELECT top 10 * FROM task_logging_stats ORDER BY CAST(QUERY_START_TIME AS DATETIME) DESC;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944b38c-c80d-4dd7-bed5-88ad6bdd4416",
   "metadata": {
    "language": "sql",
    "name": "view_dependent_tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  View all dependent task for root_task\n",
    "select *\n",
    "  from table(information_schema.task_dependents(task_name => 'root_task', recursive => false));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fdb61-bb11-49d3-8672-80745a14589d",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "#### This is the SP to drop all the tasks whose root task name is root_task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbb09f-d267-4e1f-babc-7b74e4988b23",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "sp_drop_job_tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE OR REPLACE PROCEDURE drop_job_tasks()\n",
    "RETURNS string\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "HANDLER = 'drop_tasks'\n",
    "execute as caller\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "import json\n",
    "def drop_tasks(session):\n",
    "    session.sql('alter task root_task suspend').collect()\n",
    "    res= session.sql(f''' select name\n",
    "        from table(information_schema.task_dependents(task_name => 'root_task', recursive => true))''').collect()\n",
    "    for r in res:\n",
    "        print(r.NAME)\n",
    "        session.sql(f'drop task {r.NAME}').collect()\n",
    "    session.sql('drop task GET_GRAPH_STATS').collect()\n",
    "    return 'Done'\n",
    "$$;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f701eb5-44e1-4c3d-aae3-782a970532b2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "Dropping_Tasks",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Deleting the DAG\n",
    "call drop_job_tasks();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc409e-07a4-4fe2-9977-4162bab80b5c",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "### Below code is show can you use YAML file for creating the service jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bfdd3-a483-49d3-ac8c-01d72cc7dacf",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "CreateJobService_using_yaml"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CREATE OR REPLACE PROCEDURE CreateJobService(service_name VARCHAR, pool_name VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "HANDLER = 'create_job_service'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.functions import col\n",
    "import uuid\n",
    "\n",
    "\n",
    "def create_job_service(session, service_name, pool_name):\n",
    "    import uuid\n",
    "    try:\n",
    "        # Drop the existing service if it exists\n",
    "        session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n",
    "        \n",
    "        # Execute the job service\n",
    "        session.sql(f'''\n",
    "                        EXECUTE JOB SERVICE\n",
    "                        IN COMPUTE POOL {pool_name}\n",
    "                        NAME={service_name}\n",
    "                        FROM @specs\n",
    "                        SPECIFICATION_FILE='my_job_spec.yaml' \n",
    "                    ''').collect()\n",
    "        \n",
    "        # Get the status of the job service\n",
    "        job_status = session.sql(f'''\n",
    "                                    SELECT parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n",
    "                                ''').collect()[0]['STATUS']\n",
    "        return job_status\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 'Error Occured.. Refer the logs'\n",
    "        \n",
    "                     \n",
    "$$;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
