{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_creation"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "Compute_Pools",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "show image repositories;\n\nshow compute pools like 'pr%';\n\nCREATE COMPUTE POOL pr_std_pool_xs\n  MIN_NODES = 1\n  MAX_NODES = 1\n  INSTANCE_FAMILY = CPU_X64_XS;\n\nDESCRIBE COMPUTE POOL PR_STD_POOL_XS;\n\n\nCREATE COMPUTE POOL PR_STD_POOL_S\n  MIN_NODES = 1\n  MAX_NODES = 2\n  INSTANCE_FAMILY = CPU_X64_S;\n\nshow compute pools like 'PR_STD_POOL_S';\n\ngrant usage on compute pool pr_std_pool_xs to role SPCS_PSE_ROLE;\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4b14163d-675a-4c5b-9147-4817e046ac5d",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "show compute pools like 'PR_%';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4614545-9fd4-4f7f-b719-a9b62a07d76d",
   "metadata": {
    "language": "sql",
    "name": "Resume_Pool"
   },
   "outputs": [],
   "source": "alter compute pool PR_STD_POOL_XS resume;\nalter compute pool PR_STD_POOL_S resume;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "381bfdd3-a483-49d3-ac8c-01d72cc7dacf",
   "metadata": {
    "language": "python",
    "name": "CreateJobService_using_yaml",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE PROCEDURE CreateJobService(service_name VARCHAR, pool_name VARCHAR)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_job_service'\nAS\n$$\nfrom snowflake.snowpark.functions import col\nimport uuid\n\n\ndef create_job_service(session, service_name, pool_name):\n    import uuid\n    try:\n        # Drop the existing service if it exists\n        session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n        \n        # Execute the job service\n        session.sql(f'''\n                        EXECUTE JOB SERVICE\n                        IN COMPUTE POOL {pool_name}\n                        NAME={service_name}\n                        FROM @specs\n                        SPECIFICATION_FILE='my_job_spec.yaml' \n                    ''').collect()\n        \n        # Get the status of the job service\n        job_status = session.sql(f'''\n                                    SELECT parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n                                ''').collect()[0]['STATUS']\n        return job_status\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 'Error Occured.. Refer the logs'\n        \n                     \n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d59a0e7c-4185-4ea8-8762-7fe31fe48714",
   "metadata": {
    "language": "sql",
    "name": "create_logging_table"
   },
   "outputs": [],
   "source": "-- logging individual job status\ncreate or replace table jobs_run_stats( root_task_name string, task_name string, job_status string,GRAPH_RUN_ID string , graph_start_time timestamp_ltz, errors string, created_date datetime default current_timestamp());\n\n-- Tracking all tasks part of the task graph\ncreate table task_logging_stats (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,ERROR_MESSAGE VARCHAR);\n\n-- UDTF for getting the task status for the graph - TASK_GRAPH_RUN_STATS\ncreate or replace function TASK_GRAPH_RUN_STATS(ROOT_TASK_ID string, START_TIME timestamp_ltz)\n returns table (GRAPH_RUN_GROUP_ID varchar, NAME varchar,  STATE varchar , RETURN_VALUE varchar,QUERY_START_TIME varchar,COMPLETED_TIME varchar, DURATION_IN_SECS INT,\n ERROR_MESSAGE VARCHAR)\nas\n$$\nselect\n        GRAPH_RUN_GROUP_ID,\n        NAME,\n        STATE,\n        RETURN_VALUE,\n        to_varchar(QUERY_START_TIME, 'YYYY-MM-DD HH24:MI:SS') as QUERY_START_TIME,\n        to_varchar(COMPLETED_TIME,'YYYY-MM-DD HH24:MI:SS') as COMPLETED_TIME,\n        timestampdiff('seconds', QUERY_START_TIME, COMPLETED_TIME) as DURATION,\n        ERROR_MESSAGE\n    from\n        table(INFORMATION_SCHEMA.TASK_HISTORY(\n              ROOT_TASK_ID => ROOT_TASK_ID ::string,\n              SCHEDULED_TIME_RANGE_START => START_TIME::timestamp_ltz,\n              SCHEDULED_TIME_RANGE_END => current_timestamp()\n      ))\n$$\n;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ec756b7-a57d-43d6-9b4e-5d00cdb385eb",
   "metadata": {
    "language": "sql",
    "name": "CreateJobService_spec"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE ExecuteJobService(service_name VARCHAR, pool_name VARCHAR,table_name VARCHAR,retry_count INT)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_job_service'\nAS\n$$\nfrom snowflake.snowpark.functions import col\nimport uuid\nimport re\nimport logging\nimport sys\n\nlogger = logging.getLogger(\"python_logger\")\n\ndef get_logger():\n    \"\"\"\n    Get a logger for local logging.\n    \"\"\"\n    logger = logging.getLogger(\"service-job\")\n    logger.setLevel(logging.INFO)\n    return logger\n\ndef execute_job(session, service_name, pool_name,table_name):\n   # Drop the existing service if it exists\n   session.sql(f'''DROP SERVICE if exists {service_name}''').collect()\n   session.sql(f'''\n                        EXECUTE JOB SERVICE\n                        IN COMPUTE POOL {pool_name}\n                        NAME={service_name}\n                        FROM SPECIFICATION  \n                        '\n                        spec:\n                         container:\n                         - name: main\n                           image: /pr_llmdemo/public/images/my_job_image:latest\n                           env:\n                             SNOWFLAKE_WAREHOUSE: xs_wh\n                           args:\n                           - \"--query=select current_time() as time,''hello''\"\n                           - \"--result_table={table_name}\"\n                        ';\n                    ''').collect()\n                    \n   job_status = session.sql(f'''\n                                    SELECT parse_json(SYSTEM$GET_SERVICE_STATUS('{service_name}'))[0]['status']::string as Status \n                                ''').collect()[0]['STATUS']\n   return job_status\n    \n\ndef create_job_service(session, service_name, pool_name,table_name,retry_count):\n    import uuid\n    logger = get_logger()\n    logger.info(\"job_service\")\n    try:\n\n\n        cnt = retry_count\n        job_status = ''\n\n        # Execute the job service\n        logger.info(\n            f\"Executing the Job [{service_name}] on pool [{pool_name}]\"\n        )\n        job_status = execute_job(session, service_name, pool_name,table_name)\n\n        # Implementing retry mechanism. Fetching the retry count value from the config file per job\n        if job_status=='FAILED':\n            while(cnt >0):\n                r_cnt = retry_count+1 - cnt\n                logger.info(\n                                f\"Retrying Executing the Job [{service_name}] on pool [{pool_name}] - [{r_cnt}]  out of {retry_count} times \"\n                            )\n                job_status = execute_job(session, service_name, pool_name,table_name)\n                if job_status == 'DONE':\n                    break\n                cnt = cnt - 1\n                \n        \n        if job_status=='FAILED':\n            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n            ''').collect()[0]['LOGS'])\n        else:\n            job_errors = ''\n\n        # Getting the DAG Task details. SYSTEM$TASK_RUNTIME_INFO can only work inside a task.\n        result = session.sql(\"\"\"select\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_NAME')\n                                root_task_name,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_NAME') \n                                task_name,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID')\n                                run_id,\n                                SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP')                  dag_start_time\n                            \n                            \"\"\").collect()[0]\n       \n        current_root_task_name, current_task_name ,current_graph_run_id , current_graph_start_time = result.ROOT_TASK_NAME, result.TASK_NAME ,result.RUN_ID, result.DAG_START_TIME\n        \n        #'a','b','c','2024-01-01'\n        \n        #result.ROOT_TASK_NAME, result.TASK_NAME ,result.RUN_ID, result.DAG_START_TIME\n        \n        # Inserting job status into logging table\n        _ = session.sql(f'''\n        INSERT INTO jobs_run_stats \n        (root_task_name,task_name,graph_run_id ,job_status,graph_start_time, errors ,created_date)\n        SELECT '{current_root_task_name}'\n        ,'{current_task_name}'\n        ,'{current_graph_run_id}'\n        ,'{job_status}'\n        ,'{current_graph_start_time}'\n        , '{job_errors}'\n        ,current_timestamp()\n        ''').collect()\n\n        \n        return job_status\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        if job_status=='FAILED':\n            job_errors = re.sub(r\"'\", r\"\\\\'\",session.sql(f'''\n            select SYSTEM$GET_SERVICE_LOGS('{service_name}', 0, 'main')::string as logs;\n            ''').collect()[0]['LOGS'])\n        else:\n            job_errors = ''\n        \n        session.sql(f\"\"\"\n           INSERT INTO jobs_run_stats(task_name,errors,graph_run_id,job_status,created_date)\n           SELECT '{service_name}',\n           '{job_errors}',\n           '{current_graph_run_id}',\n           '{job_status}',\n           current_timestamp()\n            \n                    \"\"\").collect()\n                    \n        return f'Error Occured.. Refer the logs - {e}'\n                   \n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddba4376-7eb6-4b58-aac2-e2cb3e3d7b73",
   "metadata": {
    "language": "sql",
    "name": "sp_create_job_tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE create_job_tasks(file_name string)\nRETURNS string\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'create_jobservice_tasks'\nAS\n$$\nfrom snowflake.snowpark.files import SnowflakeFile\nimport json\n\ndef create_jobservice_tasks(session, file_name):\n  parent_task_name = 'root_task'\n  parent_task_sql = f'''CREATE OR REPLACE TASK {parent_task_name} \n              USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL' \n              SCHEDULE = '3 MINUTE' \n      AS\n      SELECT CURRENT_TIMESTAMP() ;'''\n\n  session.sql(f'''{parent_task_sql}''').collect()\n  print(parent_task_sql)\n\n\n  with SnowflakeFile.open(file_name) as j:\n      json_data= json.load(j)\n\n\n  for idx, task in enumerate(json_data):\n      task_name = task['task_name']\n      task_sql = f\"CREATE  OR REPLACE TASK {task_name} \"\n      task_sql += f\"  WAREHOUSE = xs_wh \"\n      task_sql += f\"  AFTER {parent_task_name}  \"\n      task_sql += f\" AS CALL ExecuteJobService('{task['job_name']}','{task['compute_pool_name']}','{task['table_name']}',{task['retry_count']})\"\n      # logger.info(f'{task_sql}')\n      session.sql(f'''{task_sql}''').collect()\n\n      print(task_sql)\n\n\n  session.sql(f\"\"\"\n              create or replace task GET_GRAPH_STATS\n  warehouse = 'xs_wh'\n  finalize = 'root_task'\n  as\n    declare\n      ROOT_TASK_ID string;\n      START_TIME timestamp_ltz;\n      \n    begin\n      ROOT_TASK_ID := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_ROOT_TASK_UUID'));\n\n      START_TIME := (call SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_ORIGINAL_SCHEDULED_TIMESTAMP'));\n\n      -- Insert into the logging table\n      INSERT INTO task_logging_stats(GRAPH_RUN_GROUP_ID , NAME ,  STATE  , RETURN_VALUE ,QUERY_START_TIME ,COMPLETED_TIME , DURATION_IN_SECS ,\n                                      ERROR_MESSAGE \n                                    )\n      SELECT * FROM TABLE(TASK_GRAPH_RUN_STATS(:ROOT_TASK_ID, :START_TIME))  where NAME !='GET_GRAPH_STATS';\n\n    end;\n              \"\"\"\n              ).collect()\n\n  session.sql('alter task GET_GRAPH_STATS resume').collect()\n  \n  # session.sql(f'''alter task root_task resume''').collect()\n\n  session.sql(f'''SELECT SYSTEM$TASK_DEPENDENTS_ENABLE('root_task')''').collect()\n\n  return 'done'\n\n$$;\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "285312e4-35ed-4056-aed4-235138c2ccfb",
   "metadata": {
    "language": "sql",
    "name": "creating_job_tasks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "call create_job_tasks(build_scoped_file_url(@jobs, 'jobconfig.json'));\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb69ce30-4b3e-4b08-bd4e-77b007d12557",
   "metadata": {
    "language": "sql",
    "name": "View_Tasks"
   },
   "outputs": [],
   "source": "select *\n  from table(information_schema.task_dependents(task_name => 'root_task', recursive => false));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1114e19c-d968-42f1-8d79-286ca85e8c70",
   "metadata": {
    "language": "sql",
    "name": "Suspending_Resuming_Root_Task"
   },
   "outputs": [],
   "source": "-- alter task root_task suspend;\n-- alter task root_task resume;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e10d7c6-d664-4d32-bc48-37c1b26b1345",
   "metadata": {
    "language": "sql",
    "name": "simulating_failures"
   },
   "outputs": [],
   "source": "ALTER TABLE RESULTS_3 DROP COLUMN \"'HELLO'\";\nALTER TABLE RESULTS_3 ADD COLUMN CREATEDATE DATETIME ;\n\n-- This should fail the job3\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5fbb09f-d267-4e1f-babc-7b74e4988b23",
   "metadata": {
    "language": "sql",
    "name": "sp_drop_job_tasks"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE drop_job_tasks()\nRETURNS string\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('snowflake-snowpark-python')\nHANDLER = 'drop_tasks'\nexecute as caller\nAS\n$$\nfrom snowflake.snowpark.files import SnowflakeFile\nimport json\ndef drop_tasks(session):\n    session.sql('alter task root_task suspend').collect()\n    res= session.sql(f''' select name\n        from table(information_schema.task_dependents(task_name => 'root_task', recursive => false))''').collect()\n    for r in res:\n        print(r.NAME)\n        session.sql(f'drop task {r.NAME}').collect()\n    session.sql('drop task GET_GRAPH_STATS').collect()\n    return 'Done'\n$$;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f701eb5-44e1-4c3d-aae3-782a970532b2",
   "metadata": {
    "language": "sql",
    "name": "Dropping_Tasks",
    "collapsed": false
   },
   "outputs": [],
   "source": "call drop_job_tasks();",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2f38a8e-45dd-4e87-8dd6-5dcc54861c2a",
   "metadata": {
    "language": "sql",
    "name": "query_job_run_stats",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from jobs_run_stats order by created_date desc;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "416c9fab-b452-48c1-9ef2-e351189f3888",
   "metadata": {
    "language": "sql",
    "name": "query_task_logging_stats",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM task_logging_stats ORDER BY CAST(QUERY_START_TIME AS DATETIME) DESC;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4944b38c-c80d-4dd7-bed5-88ad6bdd4416",
   "metadata": {
    "language": "sql",
    "name": "view_dependent_tasks"
   },
   "outputs": [],
   "source": "select *\n  from table(information_schema.task_dependents(task_name => 'root_task', recursive => false));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9b8ff73-60e8-4fc6-9ed3-b03641a6b29f",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "drop table results_3;",
   "execution_count": null
  }
 ]
}